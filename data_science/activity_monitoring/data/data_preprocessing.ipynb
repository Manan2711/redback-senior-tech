{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea8efcb-6168-4419-874b-80ee66520b64",
   "metadata": {},
   "source": [
    "<br><h1 style=\"font-family:times new roman\"><center>Data Preprocessing/Preparation</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8133106-c2af-4b50-afe1-5f8d883b6386",
   "metadata": {},
   "source": [
    "In this file we will be preparing the data to be used to train the model. The idea is to prepare the data and store them in `numpy` files to store all the data in a concise and merged format. The prepartion of data inlcudes:\n",
    "1. Clipping/Padding each accelerometer and gyroscope file pair to same length and merging them into a single file.\n",
    "2. Randomly applying random data augmentations to the merged data.\n",
    "3. Splitting each merged reading into sequences of length 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065d9571-1b29-44fa-8705-d370cb342546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T12:33:23.472774Z",
     "iopub.status.busy": "2024-09-28T12:33:23.472376Z",
     "iopub.status.idle": "2024-09-28T12:33:23.904987Z",
     "shell.execute_reply": "2024-09-28T12:33:23.904565Z",
     "shell.execute_reply.started": "2024-09-28T12:33:23.472737Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_augmentation import DataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a6b66a-ac29-492e-a1a4-08db00bb4d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T12:33:23.905745Z",
     "iopub.status.busy": "2024-09-28T12:33:23.905594Z",
     "iopub.status.idle": "2024-09-28T12:33:23.917638Z",
     "shell.execute_reply": "2024-09-28T12:33:23.917073Z",
     "shell.execute_reply.started": "2024-09-28T12:33:23.905732Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_accel_gyro_data(accel_file, gyro_file):\n",
    "    \"\"\"Merge accelerometer and gyroscope data,\n",
    "    ensuring they have the same length.\n",
    "\n",
    "    Args:\n",
    "        accel_file (numpy array): Accelerometer data.\n",
    "        gyro_file (numpy array): Gyroscope data.\n",
    "\n",
    "    Returns:\n",
    "        numpy array: Merged accelerometer and gyroscope\n",
    "        data with padding (if required).\n",
    "    \"\"\"\n",
    "    # Find the minimum length between the two datasets\n",
    "    min_len = min(len(accel_file), len(gyro_file))\n",
    "\n",
    "    # Merge accelerometer and gyroscope data (ignoring the timestamp)\n",
    "    merged_data = np.hstack(\n",
    "        (accel_file[:min_len, 1:], gyro_file[:min_len, 1:]))\n",
    "\n",
    "    # Adjust the length to the nearest value divisible by 50\n",
    "    round_len = 50 * round(min_len / 50)\n",
    "\n",
    "    if round_len < min_len:\n",
    "        # Truncate the data to the rounded length\n",
    "        merged_data = merged_data[:round_len]\n",
    "    else:\n",
    "        # Add padding if needed\n",
    "        pad_len = round_len - min_len\n",
    "        pad = np.zeros(\n",
    "            (pad_len, merged_data.shape[1])\n",
    "        )  # Padding for 6 columns (3 for accel, 3 for gyro)\n",
    "        merged_data = np.vstack((pad, merged_data))\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "def split_into_sequences(merged_data, folder, user_id, trial):\n",
    "    \"\"\"Split the merged accelerometer and gyroscope data into sequences of 150-length windows.\n",
    "\n",
    "    Args:\n",
    "        merged_data (numpy array): Merged accelerometer and gyroscope data.\n",
    "        folder (str): Folder name.\n",
    "        user_id (str): User ID.\n",
    "        trial (str): Trial number.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Merged sequence data and corresponding labels.\n",
    "    \"\"\"\n",
    "    window_size = 150\n",
    "    step_size = 50\n",
    "    total_length = len(merged_data)\n",
    "\n",
    "    num_sequences = (total_length - window_size) // step_size + 1\n",
    "\n",
    "    if num_sequences <= 0:\n",
    "        # No sequences can be extracted\n",
    "        seq_data = np.empty((0, window_size, merged_data.shape[1]))\n",
    "        labels = np.empty((0, 3), dtype=object)\n",
    "        return seq_data, labels\n",
    "\n",
    "    # Generate start indices of sequences\n",
    "    start_indices = np.arange(0, num_sequences * step_size, step_size)\n",
    "\n",
    "    # Extract sequences\n",
    "    seq_data = np.array(\n",
    "        [merged_data[start: start + window_size] for start in start_indices]\n",
    "    )\n",
    "\n",
    "    # Create labels\n",
    "    labels = np.array([[folder, user_id, trial]] * len(seq_data))\n",
    "\n",
    "    return seq_data, labels\n",
    "\n",
    "\n",
    "def select_random_augmentations(num_augmentations=4):\n",
    "    \"\"\"Randomly select a specified number of augmentations from the list.\n",
    "\n",
    "    Args:\n",
    "        num_augmentations (int): Optional number of augmentations to select. Defaults to 4\n",
    "\n",
    "    Returns:\n",
    "        list: Randomly selected augmentations.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If num_augmentations exceeds the number of available augmentations.\n",
    "    \"\"\"\n",
    "\n",
    "    augmentations_list = [\n",
    "        \"add_noise\",\n",
    "        \"scaling\",\n",
    "        \"time_shift\",\n",
    "        \"flipping\",\n",
    "        \"random_cropping\",\n",
    "        \"magnitude_warping\",\n",
    "        \"cutout\",\n",
    "    ]\n",
    "\n",
    "    if num_augmentations > len(augmentations_list):\n",
    "        raise ValueError(\n",
    "            \"num_augmentations cannot be greater than the number of available augmentations.\"\n",
    "        )\n",
    "\n",
    "    selected_augmentations = random.sample(\n",
    "        augmentations_list, num_augmentations)\n",
    "\n",
    "    return selected_augmentations\n",
    "\n",
    "\n",
    "def apply_random_augmentation(merged_data, folder, user_id, trial):\n",
    "    \"\"\"Apply random augmentations to the merged data.\n",
    "\n",
    "    Args:\n",
    "        merged_data (numpy.ndarray): Merged accelerometer and gyroscope data.\n",
    "        folder (str): Folder name for identifying data.\n",
    "        user_id (str): User ID.\n",
    "        trial (str): Trial number.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            numpy.ndarray: Augmented data sequences.\n",
    "            numpy.ndarray: Corresponding labels.\n",
    "    \"\"\"\n",
    "    # Select random augmentations\n",
    "    augmentations = select_random_augmentations()\n",
    "\n",
    "    # Initialize DataAugmentation class\n",
    "    data_aug = DataAugmentation(merged_data)\n",
    "\n",
    "    # Apply selected augmentations\n",
    "    augmented_datasets = data_aug.augment_data(augmentations)\n",
    "\n",
    "    augmented_data_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for augmented_data in augmented_datasets:\n",
    "        # Process each augmented dataset\n",
    "        seq_data_augmented, labels_augmented = split_into_sequences(\n",
    "            augmented_data, folder, user_id, trial\n",
    "        )\n",
    "        augmented_data_list.append(seq_data_augmented)\n",
    "        labels_list.append(labels_augmented)\n",
    "\n",
    "        # Check if seq_data_augmented is not empty\n",
    "        if seq_data_augmented.size > 0:\n",
    "            augmented_data_list.append(seq_data_augmented)\n",
    "            labels_list.append(labels_augmented)\n",
    "\n",
    "    if augmented_data_list:\n",
    "        augmented_data = np.concatenate(augmented_data_list, axis=0)\n",
    "        labels = np.concatenate(labels_list, axis=0)\n",
    "    else:\n",
    "        augmented_data = np.empty((0, 150, merged_data.shape[1]))\n",
    "        labels = np.empty((0, 3), dtype=object)\n",
    "\n",
    "    return augmented_data, labels\n",
    "\n",
    "\n",
    "def process_data(path, augmentation_prob=0.07):\n",
    "    \"\"\"Process accelerometer and gyroscope data from files, applying random augmentations.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory path containing data folders.\n",
    "        augmentation_prob (float, optional): Probability of applying augmentation to the data.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            numpy.ndarray: Processed sequential data.\n",
    "            numpy.ndarray: Corresponding labels.\n",
    "            numpy.ndarray: Augmentation information (1 for augmented, 0 for original).\n",
    "    \"\"\"\n",
    "    sequential_data_list = []\n",
    "    sequential_label_list = []\n",
    "    augmented_info_list = []\n",
    "\n",
    "    for folder in sorted(os.listdir(path)):\n",
    "        if not folder.startswith(\"D\"):\n",
    "            continue\n",
    "\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        accel_file = None\n",
    "        gyro_file = None\n",
    "\n",
    "        for file in sorted(os.listdir(folder_path)):\n",
    "            user_id, trial = file.split(\"_\")[:2]\n",
    "            file_type = file.split(\"_\")[-1]\n",
    "\n",
    "            if file_type not in [\"accel.csv\", \"gyro.csv\"]:\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            if file_type == \"accel.csv\":\n",
    "                accel_file = pd.read_csv(file_path).to_numpy()\n",
    "            elif file_type == \"gyro.csv\":\n",
    "                gyro_file = pd.read_csv(file_path).to_numpy()\n",
    "\n",
    "            # Process once both accelerometer and gyroscope files are loaded\n",
    "            if accel_file is not None and gyro_file is not None:\n",
    "\n",
    "                merged_data = merge_accel_gyro_data(accel_file, gyro_file)\n",
    "\n",
    "                if random.random() < 0.25:\n",
    "                    pd.DataFrame(merged_data).to_csv(\"demo_plot_data.csv\")\n",
    "\n",
    "                data, labels = split_into_sequences(\n",
    "                    merged_data, folder, user_id, trial)\n",
    "\n",
    "                # Add original data and labels\n",
    "                sequential_data_list.append(data)\n",
    "                sequential_label_list.append(labels)\n",
    "                augmented_info_list.extend([0] * len(data))\n",
    "\n",
    "                # Apply random augmentation with a certain probability\n",
    "                if random.random() < augmentation_prob:\n",
    "                    augmented_data, augmented_labels = apply_random_augmentation(\n",
    "                        merged_data, folder, user_id, trial)\n",
    "\n",
    "                    sequential_data_list.append(augmented_data)\n",
    "                    sequential_label_list.append(augmented_labels)\n",
    "                    augmented_info_list.extend([1] * len(augmented_data))\n",
    "\n",
    "                # Reset accel_file and gyro_file for the next iteration\n",
    "                accel_file = None\n",
    "                gyro_file = None\n",
    "\n",
    "    # Concatenate all data and labels\n",
    "    sequential_data = np.concatenate(sequential_data_list, axis=0)\n",
    "    sequential_label = np.concatenate(sequential_label_list, axis=0)\n",
    "\n",
    "    print(\n",
    "        f\"Total Augmented Datasets – {np.sum(augmented_info_list)}/{len(augmented_info_list)}\"\n",
    "    )\n",
    "\n",
    "    return sequential_data, sequential_label\n",
    "\n",
    "\n",
    "def process_and_save_labels(\n",
    "    path,\n",
    "    data_filename=\"sequential_data.npy\",\n",
    "    label_filename=\"sequential_label.npy\",\n",
    "    label_ids_filename=\"label_ids.npy\",\n",
    "    label_to_id_filename=\"label_to_id.npy\",\n",
    "):\n",
    "    \"\"\"Process labels by mapping unique folder names to numerical IDs and saving the results to files.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory path containing data folders.\n",
    "        data_filename (str, optional): Filename to save the sequential data.\n",
    "        label_filename (str, optional): Filename to save the sequential labels.\n",
    "        label_ids_filename (str, optional): Filename to save the label IDs.\n",
    "        label_to_id_filename (str, optional): Filename to save the label-to-ID mapping.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping unique folder names to their corresponding label IDs.\n",
    "    \"\"\"\n",
    "    sequential_data, sequential_label = process_data(path)\n",
    "\n",
    "    # Extract folder names (first element in each label entry) and identify\n",
    "    # unique folders\n",
    "    labels = [x[0] for x in sequential_label]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Create a mapping of folder names to numerical label IDs\n",
    "    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Convert the original labels to numerical IDs\n",
    "    label_ids = np.array([label_to_id[label] for label in labels])\n",
    "\n",
    "    # Save the sequential data, labels, label IDs, and augmentation\n",
    "    # information as .npy files\n",
    "    np.save(data_filename, sequential_data)\n",
    "    np.save(label_filename, sequential_label)\n",
    "    np.save(label_ids_filename, label_ids)\n",
    "    np.save(label_to_id_filename, label_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0c4026-da27-486d-bb48-4822e3bf436a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-28T12:33:23.918313Z",
     "iopub.status.busy": "2024-09-28T12:33:23.918197Z",
     "iopub.status.idle": "2024-09-28T12:33:25.161338Z",
     "shell.execute_reply": "2024-09-28T12:33:25.160996Z",
     "shell.execute_reply.started": "2024-09-28T12:33:23.918303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Augmented Datasets – 5364/12159\n"
     ]
    }
   ],
   "source": [
    "PATH = \"50Hz/\"\n",
    "\n",
    "process_and_save_labels(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7557e69-20c6-4467-95bf-13ea69991595",
   "metadata": {},
   "source": [
    "Upon preparing the dataset, we can see the number of data sequences that have been augmented out of the total length of dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
